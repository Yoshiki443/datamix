{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アンダーサンプリング＆バギング\n",
    "### FGの有無を予測する（閾値1600m）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(443)\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学習用データ\n",
    "DATA = pd.read_csv(\"train_data.csv\")\n",
    "\n",
    "# 使うデータを限定\n",
    "use_valiable = [\"DateTime\",\"VIS\",\"VIS_CAT\",\"FG\",\"PRCP_P24HR\",\\\n",
    "                \"RH_SFC\",\"TMP_SFC\",\"TD_SFC\",\"PRES_SFC\",\"LCDC_SFC\",\"MCDC_SFC\",\"HCDC_SFC\",\\\n",
    "                \"WSPD_SFC\",\"WDIR_SFC\",\"APCP_SFC\",\"TimeRange\",\"MONTH\",\\\n",
    "#                 \"D_PRES_SFC\",\"D_TMP_SFC\",\"D_TD_SFC\",\\\n",
    "                \"LL_VWS1\",\"LL_VWS2\",\"LL_STBL1\",\"LL_STBL2\",\"WARMER_RA\",\\\n",
    "                \"RH_1000\",\"VVEL_1000\",\"WSPD_1000\",\"RH_975\",\"VVEL_975\",\"WSPD_975\",\\\n",
    "                \"RH_950\",\"VVEL_950\",\"WSPD_950\",\"RH_850\",\"RH_700\",\"RH_500\",\"RH_300\"]\n",
    "# DATA = DATA[use_valiable]\n",
    "DATA = DATA[use_valiable].drop(\"DateTime\", axis=1)\n",
    "\n",
    "# カテゴリー変数はダミー化\n",
    "cat_val = ['WDIR_SFC', 'TimeRange', 'MONTH']\n",
    "DATA = pd.get_dummies(data=DATA, columns=cat_val)\n",
    "\n",
    "# 雨が降った後の夜間に霧が出やすいことを表現できるかもしれない\n",
    "DATA[\"Time_12-14_RAp24hr\"] = DATA[\"PRCP_P24HR\"] * DATA[\"TimeRange_12-14\"]\n",
    "DATA[\"Time_15-17_RAp24hr\"] = DATA[\"PRCP_P24HR\"] * DATA[\"TimeRange_15-17\"]\n",
    "DATA[\"Time_18-20_RAp24hr\"] = DATA[\"PRCP_P24HR\"] * DATA[\"TimeRange_18-20\"]\n",
    "DATA[\"Time_21-23_RAp24hr\"] = DATA[\"PRCP_P24HR\"] * DATA[\"TimeRange_21-23\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 予測テスト用データ\n",
    "TEST = pd.read_csv(\"test_data.csv\")\n",
    "TEST = TEST[use_valiable].drop(\"DateTime\", axis=1)\n",
    "TEST = pd.get_dummies(data=TEST, columns=cat_val)\n",
    "\n",
    "TEST[\"Time_12-14_RAp24hr\"] = TEST[\"PRCP_P24HR\"] * TEST[\"TimeRange_12-14\"]\n",
    "TEST[\"Time_15-17_RAp24hr\"] = TEST[\"PRCP_P24HR\"] * TEST[\"TimeRange_15-17\"]\n",
    "TEST[\"Time_18-20_RAp24hr\"] = TEST[\"PRCP_P24HR\"] * TEST[\"TimeRange_18-20\"]\n",
    "TEST[\"Time_21-23_RAp24hr\"] = TEST[\"PRCP_P24HR\"] * TEST[\"TimeRange_21-23\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'FG'\n",
    "exclude = ['VIS','VIS_CAT','FG','PRCP_P24HR']\n",
    "features = [val for val in DATA.columns if val not in exclude]\n",
    "\n",
    "# 特徴量を割り算で作成\n",
    "cutoff_r = 0.5\n",
    "new_added_col = []\n",
    "for i in range(0, len(features)-1):\n",
    "    for j in range(i+1, len(features)):\n",
    "        first_col_name = features[i]\n",
    "        second_col_name = features[j]\n",
    "        r = spearmanr(DATA[first_col_name], DATA[second_col_name]).correlation        \n",
    "        if abs(r) > cutoff_r:\n",
    "            new_colname = first_col_name + \"_div_\" + second_col_name\n",
    "            DATA[new_colname] = DATA[first_col_name] / (DATA[second_col_name] + 0.001)\n",
    "            new_added_col.append(new_colname)\n",
    "features = features + new_added_col\n",
    "\n",
    "# 増えた特徴量をテストデータでも作成\n",
    "for feature in features:\n",
    "    if re.search('_div_', feature):\n",
    "        feature1, feature2 = feature.split(\"_div_\")\n",
    "        TEST[feature] = TEST[feature1] / (TEST[feature2] + 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26181, 125), (8137, 125))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.shape, TEST.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestのハイパーパラメータを最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling関数\n",
    "# https://qiita.com/ryouta0506/items/619d9ac0d80f8c0aed92\n",
    "# Undersamplingするとき、まずクラスタリングし、\n",
    "# 各クラスターの標本数と同じ割合で、各クラスターからサンプリングする\n",
    "\n",
    "# Under Samplingの関数（X:サンダーサンプルするデータ num:アンダーサンプリング数 label:多数派のラベル）\n",
    "def Undersampling_Kmeans(X,num,label) :\n",
    "    \n",
    "    # KMeansによるクラスタリング\n",
    "    from sklearn.cluster import KMeans\n",
    "    km = KMeans(n_clusters=8, init=\"k-means++\")\n",
    "    km.fit(X)\n",
    "    X['Cluster'] = km.predict(X)\n",
    "\n",
    "    # 群別の構成比を少数派の件数に乗じて群別の抽出件数を計算\n",
    "    count_sum = X.groupby('Cluster').count().iloc[0:,0].as_matrix()\n",
    "    ratio = count_sum / count_sum.sum()\n",
    "    #print( ratio )\n",
    "    samp_num = np.round(ratio * num,0).astype(np.int32)\n",
    "\n",
    "    # 群別にサンプリング処理を実施\n",
    "    for i in np.arange(8) :\n",
    "        tmp = X[X['Cluster']==i]\n",
    "        if i == 0 :\n",
    "            tmp1 = tmp.sample(samp_num[i],replace=True)\n",
    "        else :\n",
    "            tmp2 = tmp.sample(samp_num[i],replace=True)\n",
    "            tmp1 = pd.concat([tmp1,tmp2])\n",
    "    return tmp1.drop(\"Cluster\", axis=1)\n",
    "\n",
    "### クラスタリングも合わせたUndersampling\n",
    "def Clustered_Undersampling( data, col_name ):\n",
    "    data0 = data[ data[col_name] == 0 ]\n",
    "    data1 = data[ data[col_name] == 1 ]\n",
    "\n",
    "    X0 = data0.drop(col_name, axis=1)\n",
    "    data0 = Undersampling_Kmeans( X0, data1.shape[0], 0 )\n",
    "    data0[col_name] = 0\n",
    "    \n",
    "    return pd.concat( [data0, data1], ignore_index=True )\n",
    "\n",
    "### 通常のUndersampling\n",
    "def Normal_Undersampling( data, target ):\n",
    "    features = [val for val in data.columns if val not in target]\n",
    "    y = np.array( data[target] )\n",
    "    X = np.array( data[features] )\n",
    "\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    rus = RandomUnderSampler(ratio='not minority') \n",
    "    X_rus, y_rus = rus.fit_sample(X,y)\n",
    "    \n",
    "    new_df = pd.DataFrame( X_rus, columns=features )\n",
    "    new_df[target] = y_rus\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 0\n",
      "{'max_depth': 3, 'max_features': 10, 'n_estimators': 1000}\n",
      "0.8452380952380952\n",
      "Loop 1\n",
      "{'max_depth': 3, 'max_features': 20, 'n_estimators': 1000}\n",
      "0.8544973544973545\n",
      "Loop 2\n",
      "{'max_depth': 3, 'max_features': 10, 'n_estimators': 5000}\n",
      "0.8492063492063492\n",
      "Loop 3\n",
      "{'max_depth': 3, 'max_features': 20, 'n_estimators': 1000}\n",
      "0.8505291005291006\n",
      "Loop 4\n",
      "{'max_depth': 3, 'max_features': 10, 'n_estimators': 1000}\n",
      "0.8465608465608465\n",
      "Loop 5\n",
      "{'max_depth': 3, 'max_features': 20, 'n_estimators': 1000}\n",
      "0.8518518518518519\n",
      "Loop 6\n",
      "{'max_depth': 3, 'max_features': 20, 'n_estimators': 1000}\n",
      "0.8386243386243386\n",
      "Loop 7\n",
      "{'max_depth': 3, 'max_features': 20, 'n_estimators': 1000}\n",
      "0.8584656084656085\n",
      "Loop 8\n",
      "{'max_depth': 3, 'max_features': 20, 'n_estimators': 5000}\n",
      "0.8412698412698413\n",
      "Loop 9\n",
      "{'max_depth': 3, 'max_features': 10, 'n_estimators': 1000}\n",
      "0.832010582010582\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print( \"Loop {}\" . format(i) )\n",
    "    \n",
    "    ## Undersampling\n",
    "    # DATA_us = Clustered_Undersampling( DATA, \"FG\" )\n",
    "    DATA_us = Normal_Undersampling( DATA, \"FG\" )\n",
    "\n",
    "    X = np.array( DATA_us[features] )\n",
    "    y = np.array( DATA_us[target] )\n",
    "\n",
    "    ## グリッドサーチでハイパーパラメータのチューニング\n",
    "    rf = RandomForestClassifier(class_weight='balanced', random_state=443)\n",
    "    params = {'n_estimators' :[1000, 5000], 'max_depth':[3], 'max_features':[10, 20]}\n",
    "#     gcv = GridSearchCV(rf, param_grid=params, n_jobs=-1, cv=3, scoring='roc_auc', verbose=0)\n",
    "#     gcv = GridSearchCV(rf, param_grid=params, n_jobs=-1, cv=3, scoring='f1', verbose=0)\n",
    "    gcv = GridSearchCV(rf, param_grid=params, n_jobs=-1, cv=3, scoring='accuracy', verbose=0)\n",
    "    gcv.fit(X, y)\n",
    "    print( gcv.best_params_ )\n",
    "    print( gcv.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'n_estimators': 1000\n",
    "# 'max_depth': 3\n",
    "# 'max_features': 20\n",
    "\n",
    "# RondomForestの設定\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=3, max_features=20, random_state=443)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スレットスコアが最も高くなる閾値を求める\n",
    "def Best_TS_Cutoff(y_test, y_proba):\n",
    "    accuracy = []\n",
    "    for i in range(1,100):\n",
    "        var = i / 100\n",
    "        ypred_flag = (y_proba[:,1] > var).astype(np.int)\n",
    "        m = confusion_matrix(y_true=y_test, y_pred=ypred_flag)\n",
    "        ts = m[1,1] / ( m[0,1] + m[1,0] + m[1,1] )\n",
    "        accuracy.append( ts )\n",
    "    return (np.argsort(accuracy)[-1] + 1) / 100\n",
    "\n",
    "# Equitable Threat Scoreを使う\n",
    "def Best_ETS_Cutoff(y_test, y_proba):\n",
    "    Pc = len(y_test[ y_test == 1 ]) / len(y_test) # 気候学的出現率\n",
    "    accuracy = []\n",
    "    for i in range(1,100):\n",
    "        var = i / 100\n",
    "        ypred_flag = (y_proba[:,1] > var).astype(np.int)\n",
    "        m = confusion_matrix(y_true=y_test, y_pred=ypred_flag)\n",
    "        Sf = Pc * ( m[1,1] + m[0,1] ) # ランダム的中率\n",
    "        ets = ( m[1,1] - Sf ) / ( m[0,1] + m[1,0] + m[1,1] - Sf )\n",
    "        accuracy.append( ets )\n",
    "    return (np.argsort(accuracy)[-1] + 1) / 100\n",
    "\n",
    "# 評価計算を出力\n",
    "def Evaluation(y_test, y_proba):\n",
    "    #cutoff = Best_TS_Cutoff( y_test, y_proba )\n",
    "    cutoff = Best_ETS_Cutoff( y_test, y_proba )\n",
    "    y_fcst = (y_proba[:,1] > cutoff).astype(np.int)\n",
    "\n",
    "    print( \"AUC : {}\" . format( roc_auc_score(y_test, y_proba[:,1]) ) )\n",
    "    print( \"\" )\n",
    "    print( \"cutoff : {}\" . format(cutoff) )\n",
    "    print( classification_report(y_true=y_test, y_pred=y_fcst) )\n",
    "    matrix = confusion_matrix(y_true=y_test, y_pred=y_fcst)\n",
    "    print( \"Confusion Matrix\" )\n",
    "    print( matrix )\n",
    "    print( \"\" )\n",
    "    print( \"Threat Score : {}\" . format( matrix[1,1]/(matrix[0,1]+matrix[1,0]+matrix[1,1]) ) )\n",
    "    Pc = len(y_test[ y_test == 1 ]) / len(y_test)\n",
    "    Sf = Pc * ( matrix[1,1] + matrix[0,1] )\n",
    "    print( \"ETS : {}\" . format( (matrix[1,1]-Sf)/(matrix[0,1]+matrix[1,0]+matrix[1,1]-Sf) ) )\n",
    "    print( \"BI : {}\" . format( matrix[:,1].sum() / matrix[1,:].sum() ) )\n",
    "    return cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット\n",
    "X = np.array( DATA[features] )\n",
    "Y = np.array( DATA[target] )\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.3, random_state=443)\n",
    "\n",
    "X_test = np.array( TEST[features] )\n",
    "Y_test = np.array( TEST[target] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] n_estimators=250 ................................................\n",
      "[CV] ................................. n_estimators=100, total= 6.1min\n",
      "[CV] n_estimators=250 ................................................\n",
      "[CV] ................................. n_estimators=100, total= 6.1min\n",
      "[CV] n_estimators=250 ................................................\n",
      "[CV] ................................. n_estimators=100, total= 6.2min\n",
      "[CV] n_estimators=500 ................................................\n",
      "[CV] ................................. n_estimators=250, total=15.3min\n",
      "[CV] n_estimators=500 ................................................\n",
      "[CV] ................................. n_estimators=250, total=15.3min\n",
      "[CV] n_estimators=500 ................................................\n",
      "[CV] ................................. n_estimators=250, total=15.3min\n",
      "[CV] ................................. n_estimators=500, total=28.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed: 45.0min remaining: 12.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................. n_estimators=500, total=26.9min\n",
      "[CV] ................................. n_estimators=500, total=24.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 58.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500}\n",
      "0.14103767607738887\n"
     ]
    }
   ],
   "source": [
    "# Undersampling & Bagging\n",
    "bbc = BalancedBaggingClassifier(base_estimator=rf, ratio='not minority', random_state=443)\n",
    "\n",
    "params = {'n_estimators' :[100, 250, 500]}\n",
    "gcv = GridSearchCV(bbc, param_grid=params, n_jobs=-1, cv=3, scoring='f1', verbose=2)\n",
    "gcv.fit(X, Y)\n",
    "\n",
    "print( gcv.best_params_ )\n",
    "print( gcv.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC : 0.9293301727479715\n",
      "\n",
      "cutoff : 0.82\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98      7739\n",
      "          1       0.21      0.47      0.29       116\n",
      "\n",
      "avg / total       0.98      0.97      0.97      7855\n",
      "\n",
      "Confusion Matrix\n",
      "[[7537  202]\n",
      " [  61   55]]\n",
      "\n",
      "Threat Score : 0.17295597484276728\n",
      "ETS : 0.1629660812988893\n",
      "BI : 2.2155172413793105\n"
     ]
    }
   ],
   "source": [
    "bbc = BalancedBaggingClassifier(base_estimator=rf, n_estimators=100, ratio='not minority', n_jobs=-1, random_state=443)\n",
    "bbc.fit(X_train, Y_train)\n",
    "y_proba = bbc.predict_proba(X_valid)\n",
    "cutoff = Evaluation( Y_valid, y_proba )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Accuracy(y_test, y_proba, cutoff):\n",
    "    y_fcst = (y_proba[:,1] > cutoff).astype(np.int)\n",
    "\n",
    "    print( \"AUC : {}\" . format( roc_auc_score(y_test, y_proba[:,1]) ) )\n",
    "    print( \"\" )\n",
    "    print( \"cutoff : {}\" . format(cutoff) )\n",
    "    print( classification_report(y_true=y_test, y_pred=y_fcst) )\n",
    "    matrix = confusion_matrix(y_true=y_test, y_pred=y_fcst)\n",
    "    print( \"Confusion Matrix\" )\n",
    "    print( matrix )\n",
    "    print( \"\" )\n",
    "    print( \"Threat Score : {}\" . format( matrix[1,1]/(matrix[0,1]+matrix[1,0]+matrix[1,1]) ) )\n",
    "    Pc = len(y_test[ y_test == 1 ]) / len(y_test)\n",
    "    Sf = Pc * ( matrix[1,1] + matrix[0,1] )\n",
    "    print( \"ETS : {}\" . format( (matrix[1,1]-Sf)/(matrix[0,1]+matrix[1,0]+matrix[1,1]-Sf) ) )\n",
    "    print( \"BI : {}\" . format( matrix[:,1].sum() / matrix[1,:].sum() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC : 0.9022949072712695\n",
      "\n",
      "cutoff : 0.82\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.97      0.98      8038\n",
      "          1       0.16      0.37      0.22        99\n",
      "\n",
      "avg / total       0.98      0.97      0.97      8137\n",
      "\n",
      "Confusion Matrix\n",
      "[[7837  201]\n",
      " [  62   37]]\n",
      "\n",
      "Threat Score : 0.12333333333333334\n",
      "ETS : 0.11478909535237915\n",
      "BI : 2.404040404040404\n"
     ]
    }
   ],
   "source": [
    "# 予測テスト\n",
    "y_proba = bbc.predict_proba(X_test)\n",
    "Final_Accuracy( Y_test, y_proba, cutoff )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
